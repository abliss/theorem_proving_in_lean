#+Title: Theorem Proving in Lean
#+Author: [[http://www.andrew.cmu.edu/user/avigad][Jeremy Avigad]]

* Quantifiers and Equality
:PROPERTIES:
  :CUSTOM_ID: Quantifiers_and_Equality
:END:

The last chapter introduced you to methods that construct proofs of
statements involving the propositional connectives. In this chapter,
we extend the repertoire of logical constructions to include the
universal and existential quantifiers, and the equality relation.

** The Universal Quantifier

Notice that if =α= is any type, we can represent a unary predicate =p=
on =α= as an object of type =α → Prop=. In that case, given =x : α=,
=p x= denotes the assertion that =p= holds of =x=. Similarly, an
object =r : α → α → Prop= denotes a binary relation on =α=: given =x
y : α=, =r x y= denotes the assertion that =x= is related to =y=.

The universal quantifier, =∀ x : α, p x= is supposed to denote the
assertion that "for every =x : α=, =p x=" holds. As with the
propositional connectives, in systems of natural deduction, "forall"
is governed by an introduction and elimination rule. Informally, the
introduction rule states:
#+BEGIN_QUOTE
Given a proof of =p x=, in a context where =x : α= is arbitrary, we
obtain a proof =∀ x : α, p x=.
#+END_QUOTE
The elimination rule states:
#+BEGIN_QUOTE
Given a proof =∀ x : α, p x= and any term =t : α=, we obtain a proof of
=p t=.
#+END_QUOTE
As was the case for implication, the propositions-as-types
interpretation now comes into play. Remember the introduction and
elimination rules for Pi types:
#+BEGIN_QUOTE
Given a term =t= of type =β x=, in a context where =x : α= is
arbitrary, we have =(λ x : α, t) : Π x : α, β x=.
#+END_QUOTE
The elimination rule states:
#+BEGIN_QUOTE
Given a term =s : Π x : α, β x= and any term =t : α=, we have =s t : β t=.
#+END_QUOTE
In the case where =p x= has type =Prop=, if we replace =Π x : α, β x=
with =∀ x : α, p x=, we can read these as the correct rules for
building proofs involving the universal quantifier.

The Calculus of Inductive Constructions therefore identifies =Π= and
=∀= in this way. If =p= is any expression, =∀ x : α, p= is nothing more
than alternative notation for =Π x : α, p=, with the idea that the
former is more natural than the latter in cases where where =p= is a
proposition. Typically, the expression =p= will depend on =x :
α=. Recall that, in the case of ordinary function spaces, we could
interpret =α → β= as the special case of =Π x : α, β= in which =β= does
not depend on =x=. Similarly, we can think of an implication =p → q=
between propositions as the special case of =∀ x : p, q= in which the
expression =q= does not depend on =x=.

Here is an example of how the propositions-as-types correspondence
gets put into practice.
#+BEGIN_SRC lean
variables (α : Type) (p q : α → Prop)

example : (∀ x : α, p x ∧ q x) → ∀ y : α, p y  :=
assume h : ∀ x : α, p x ∧ q x,
take y : α,
show p y, from (h y).left
#+END_SRC

As a notational convention, we give the universal quantifier the
widest scope possible, so parentheses are needed to limit the
quantifier over =x= to the hypothesis in the example above. The
canonical way to prove =∀ y : α, p y= is to take an arbitrary =y=, and
prove =p y=. This is the introduction rule. Now, given that =h= has
type =∀ x : α, p x ∧ q x=, the expression =h y= has type =p y ∧ q
y=. This is the elimination rule. Taking the left conjunct gives the
desired conclusion, =p y=.

Remember that expressions which differ up to renaming of bound
variables are considered to be equivalent. So, for example, we could
have used the same variable, =x=, in both the hypothesis and
conclusion, and instantiated it by a different variable, =z=, in the
proof:
#+BEGIN_SRC lean
variables (α : Type) (p q : α → Prop)

-- BEGIN
example : (∀ x : α, p x ∧ q x) → ∀ x : α, p x  :=
assume h : ∀ x : α, p x ∧ q x,
take z : α,
show p z, from and.elim_left (h z)
-- END
#+END_SRC

As another example, here is how we can express the fact that a
relation, =r=, is transitive:
#+BEGIN_SRC lean
variables (α : Type) (r : α → α → Prop)
variable  trans_r : ∀ x y z, r x y → r y z → r x z

variables a b c : α
variables (hab : r a b) (hbc : r b c)

#check trans_r    -- ∀ (x y z : α), r x y → r y z → r x z
#check trans_r a b c
#check trans_r a b c hab
#check trans_r a b c hab hbc
#+END_SRC
Think about what is going on here. When we instantiate =trans_r= at
the values =a b c=, we end up with a proof of =r a b → r b c → r a
c=. Applying this to the "hypothesis" =hab : r a b=, we get a proof of
the implication =r b c → r a c=. Finally, applying it to the
hypothesis =hbc= yields a proof of the conclusion =r a c=.

In situations like this, it can be tedious to supply the arguments =a
b c=, when they can be inferred from =hab hbc=. For that reason, it is
common to make these arguments implicit:
#+BEGIN_SRC lean
universe u
variables (α : Type u) (r : α → α → Prop)
variable  trans_r : ∀ {x y z}, r x y → r y z → r x z

variables (a b c : α)
variables (hab : r a b) (hbc : r b c)

#check trans_r
#check trans_r hab
#check trans_r hab hbc
#+END_SRC
The advantage is that we can simply write =trans_r hab hbc= as a proof
of =r a c=. A disadvantage is that Lean does not have enough
information to infer the types of the arguments in the expressions
=trans_r= and =trans_r hab=. The output of the first =#check= command
is =r ?M_1 ?M_2 → r ?M_2 ?M_3 → r ?M_1 ?M_3=, indicating that the
implicit arguments are unspecified in this case.

Here is an example of how we can carry out elementary reasoning with
an equivalence relation:
#+BEGIN_SRC lean
variables (α : Type) (r : α → α → Prop)

variable refl_r : ∀ x, r x x
variable symm_r : ∀ {x y}, r x y → r y x
variable trans_r : ∀ {x y z}, r x y → r y z → r x z

example (a b c d : α) (hab : r a b) (hcb : r c b) (hcd : r c d) : r a d :=
trans_r (trans_r hab (symm_r hcb)) hcd
#+END_SRC
To get used to using universal quantifiers, you should try some of the
exercises at the end of this section.

It is the typing rule for Pi types, and the universal quantifier in
particular, that distinguishes =Prop= from other types. Suppose we
have =α : Sort i= and =β : Sort j=, where the expression =β= may
depend on a variable =x : α=. Then =Π x : α, β= is an element of
=Type (imax i j)=, where =imax i j= is the maximum of =i= and =j= if
=j= is not 0, and 0 otherwise.

The idea is as follows. If =j= is not =0=, then =Π x : α, β= is an
element of =Sort (max i j)=. In other words, the type of dependent
functions from =α= to =β= "lives" in the universe whose index is the
maximum of =i= and =j=.  Suppose, however, that =β= is of =Sort 0=,
that is, an element of =Prop=. In that case, =Π x : α, β= is an
element of =Sort 0= as well, no matter which type universe =α= lives
in. In other words, if =β= is a proposition depending on =α=, then =∀
x : α, β= is again a proposition. This reflects the interpretation of
=Prop= as the type of propositions rather than data, and it is what
makes =Prop= /impredicative/.

The term "predicative" stems from foundational developments around the
turn of the twentieth century, when logicians such as Poincaré and
Russell blamed set-theoretic paradoxes on the "vicious circles" that
arise when we define a property by quantifying over a collection that
includes the very property being defined. Notice that if =α= is any
type, we can form the type =α → Prop= of all predicates on =α= (the
"power type of =α="). The impredicativity of Prop means that we can
form propositions that quantify over =α → Prop=. In particular, we can
define predicates on =α= by quantifying over all predicates on =α=,
which is exactly the type of circularity that was once considered
problematic.

** Equality
:PROPERTIES:
  :CUSTOM_ID: Equality
:END:

Let us now turn to one of the most fundamental relations defined in
Lean's library, namely, the equality relation. In [[file:07_Inductive_Types.org::#Inductive_Types][Chapter 7]], we will
explain /how/ equality is defined from the primitives of Lean's
logical framework. In the meanwhile, here we explain how to use it.

Of course, a fundamental property of equality is that it is an
equivalence relation:
#+BEGIN_SRC lean
#check eq.refl    -- ∀ (a : ?M_1), a = a
#check eq.symm    -- ?M_2 = ?M_3 → ?M_3 = ?M_2
#check eq.trans   -- ?M_2 = ?M_3 → ?M_3 = ?M_4 → ?M_2 = ?M_4
#+END_SRC
We can make the output easier to read by telling Lean not to the insert
implicit arguments (which are displayed here as metavariables).
#+BEGIN_SRC lean
universe u

#check @eq.refl.{u}   -- ∀ {α : Sort u} (a : α), a = a
#check @eq.symm.{u}   -- ∀ {α : Sort u} {a b : α}, a = b → b = a
#check @eq.trans.{u}  -- ∀ {α : Sort u} {a b c : α}, a = b → b = c → a = c
#+END_SRC
The inscription =.{u}= tells Lean to instantiate the constants at the
universe =u=.

Thus, for example, we can specialize the example from the previous section
to the equality relation:
#+BEGIN_SRC lean
universe u
variables (α : Type) (a b c d : α)
variables (hab : a = b) (hcb : c = b) (hcd : c = d)

example : a = d :=
eq.trans (eq.trans hab (eq.symm hcb)) hcd
#+END_SRC
If we "open" the =eq= namespace, the names become shorter:
#+BEGIN_SRC lean
universe u
variables (α : Type u) (a b c d : α)
variables (hab : a = b) (hcb : c = b) (hcd : c = d)

-- BEGIN
open eq

example : a = d := trans (trans hab (symm hcb)) hcd
-- END
#+END_SRC
We can also use the projection notation:
#+BEGIN_SRC lean
universe u
variables (α : Type u) (a b c d : α)
variables (hab : a = b) (hcb : c = b) (hcd : c = d)

-- BEGIN
example : a = d := (hab.trans hcb.symm).trans hcd
-- END
#+END_SRC

Reflexivity is more powerful than it looks. Recall that terms in the
Calculus of Inductive Constructions have a computational
interpretation, and that the logical framework treats terms with a
common reduct as the same. As a result, some nontrivial identities can
be proved by reflexivity:
#+BEGIN_SRC lean
universe u
variables (α β : Type u)

example (f : α → β) (a : α) : (λ x, f x) a = f a := eq.refl _
example (a : α) (b : α) : (a, b).1 = a := eq.refl _
example : 2 + 3 = 5 := eq.refl _
#+END_SRC
This feature of the framework is so important that the library defines
a notation =rfl= for =eq.refl _=:
#+BEGIN_SRC lean
universe u
variables (α β : Type u)

-- BEGIN
example (f : α → β) (a : α) : (λ x, f x) a = f a := rfl
example (a : α) (b : α) : (a, b).1 = a := rfl
example : 2 + 3 = 5 := rfl
-- END
#+END_SRC

Equality is much more than an equivalence relation, however. It has
the important property that every assertion respects the equivalence,
in the sense that we can substitute equal expressions without changing
the truth value. That is, given =h1 : a = b= and =h2 : p a=, we can construct
a proof for =p b= using substitution: =eq.subst h1 h2=.
#+BEGIN_SRC lean
universe u

example (α : Type u) (a b : α) (p : α → Prop) (h1 : a = b) (h2 : p a) : p b :=
eq.subst h1 h2

example (α : Type u) (a b : α) (p : α → Prop) (h1 : a = b) (h2 : p a) : p b :=
h1 ▸ h2
#+END_SRC
The triangle in the second presentation is nothing more than notation
for =eq.subst=, and you can enter it by typing =\t=.

The rule =eq.subst= is used to define the following auxiliary rules,
which carry out more explicit substitutions. They are designed to deal
with applicative terms, that is, terms of form =s t=. Specifically,
=congr_arg= can be used to replace the argument, =congr_fun= can be
used to replace the terms that is being applied, and =congr= can be
used to replace both at once.
#+BEGIN_SRC lean
variable α : Type
variables a b : α
variables f g : α → ℕ
variable h₁ : a = b
variable h₂ : f = g

example : f a = f b := congr_arg f h₁
example : f a = g a := congr_fun h₂ a
example : f a = g b := congr h₂ h₁
#+END_SRC

Lean's library contains a large number of common identities, such as
these:
#+BEGIN_SRC lean
variables a b c d : ℤ

example : a + 0 = a := add_zero a
example : 0 + a = a := zero_add a
example : a * 1 = a := mul_one a
example : 1 * a = a := one_mul a
example : -a + a = 0 := neg_add_self a
example : a + -a = 0 := add_neg_self a
example : a - a = 0 := sub_self a
example : a + b = b + a := add_comm a b
example : a + b + c = a + (b + c) := add_assoc a b c
example : a * b = b * a := mul_comm a b
example : a * b * c = a * (b * c) := mul_assoc a b c
example : a * (b + c) = a * b + a * c := mul_add a b c
example : a * (b + c) = a * b + a * c := left_distrib a b c -- alternative name
example : (a + b) * c = a * c + b * c := add_mul a b c
example : (a + b) * c = a * c + b * c := right_distrib a b c -- alternative name
example : a * (b - c) = a * b - a * c := mul_sub a b c
example : (a - b) * c = a * c - b * c := sub_mul a b c
#+END_SRC
Here we state these properties for the integers; the type =ℤ= can be
entered as =\int=, though we can also use the ascii equivalent
=int=. Identities likes these are designed to work in arbitrary
instances of the relevant algebraic structures, using the type class
mechanism that is described in [[file:10_Type_Classes.org::#Type_Classes][Chapter 10]]. In particular, all these
facts hold in any commutative ring, of which Lean recognizes the
integers to be an instance. [[file::06_Interacting_with_Lean.org:#Interacting_with_Lean.org][Chapter 6]] provides some pointers as to how
to find theorems like this in the library.

Here is an example of a calculation in the natural numbers that uses
substitution combined with associativity, commutativity, and
distributivity of the integers.
#+BEGIN_SRC lean
variables x y z : ℤ

example (x y z : ℕ) : x * (y + z) = x * y + x * z := mul_add x y z
example (x y z : ℕ) : x * (y + z) = x * y + x * z := mul_add x y z
example (x y z : ℕ) : x + y + z = x + (y + z) := add_assoc x y z

example (x y : ℕ) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
have h1 : (x + y) * (x + y) = (x + y) * x + (x + y) * y, from mul_add (x + y) x y,
have h2 : (x + y) * (x + y) = x * x + y * x + (x * y + y * y),
  from (add_mul x y x) ▸ (add_mul x y y) ▸ h1,
h2.trans (add_assoc (x * x + y * x) (x * y) (y * y)).symm
#+END_SRC

Notice that the second implicit parameter to =eq.subst=, which
provides the context in which the substitution is to occur, has type
=α → Prop=. Inferring this predicate therefore requires an instance of
/higher-order unification/. In full generally, the problem of
determining whether a higher-order unifier exists is undecidable, and
Lean can at best provide imperfect and approximate solutions to the
problem. As a result, =eq.subst= doesn't always do what you want it
to. This issue is discussed in greater detail in [[file:06_Interacting_with_Lean.org::#Elaboration_Hints][Section 6.10]]. 

Because equational reasoning is so common and important, Lean provides
a number of mechanisms to carry it out more effectively. The next
section offers syntax that allow you to write calculational proofs in
a more natural and perspicuous way. But, more importantly, equational
reasoning is supported by a term rewriter, a simplifier, and other
kinds of automation. The term rewriter and simplifier are described
briefly in the next setion, and then in greater detail in the next
chapter.

** Calculational Proofs
:PROPERTIES:
  :CUSTOM_ID: Calculational_Proofs
:END:

A calculational proof is just a chain of intermediate results that are
meant to be composed by basic principles such as the transitivity of
equality. In Lean, a calculation proof starts with the keyword =calc=,
and has the following syntax:
#+BEGIN_SRC text
calc
  <expr>_0  'op_1'  <expr>_1  ':'  <proof>_1
    '...'   'op_2'  <expr>_2  ':'  <proof>_2
     ...
    '...'   'op_n'  <expr>_n  ':'  <proof>_n
#+END_SRC
Each =<proof>_i= is a proof for =<expr>_{i-1} op_i <expr>_i=. The
=<proof>_i= may also be of the form ={ <pr> }=, where =<pr>= is a
proof for some equality =a = b=. The form ={ <pr> }= is just syntactic
sugar for =eq.subst <pr> (eq.refl <expr>_{i-1})= In other words, we are
claiming we can obtain =<expr>_i= by replacing =a= with =b= in
=<expr>_{i-1}=.

Here is an example:
#+BEGIN_SRC lean
variables (a b c d e : ℕ)
variable h1 : a = b
variable h2 : b = c + 1
variable h3 : c = d
variable h4 : e = 1 + d

theorem T : a = e :=
calc
  a     = b      : h1
    ... = c + 1  : h2
    ... = d + 1  : congr_arg _ h3
    ... = 1 + d  : add_comm d (1 : ℕ)
    ... =  e     : eq.symm h4
#+END_SRC
The style of writing proofs is most effective when it is used in
conjunction with the =simp= and =rewrite= tactics, which are discussed
in greater detail in the next chapter. For example, using the
abbreviation =rw= for rewrite, the proof above could be written as
follows:
#+BEGIN_SRC lean
variables (a b c d e : ℕ)
variable h1 : a = b
variable h2 : b = c + 1
variable h3 : c = d
variable h4 : e = 1 + d

include h1 h2 h3 h4
theorem T : a = e :=
calc
  a     = b      : by rw h1
    ... = c + 1  : by rw h2
    ... = d + 1  : by rw h3
    ... = 1 + d  : by rw add_comm
    ... =  e     : by rw h4
#+END_SRC
In the next chapter, we will see that hypotheses can be introduced,
renamed, and modified by tactics, so it is not always clear what the
names in =rw h1= refer to (though, in this case, it is). For that
reason, section variables and variables that only appear in a tactic command
or block are not automatically add to the context. The =include=
command takes care of that. Essentially, the =rewrite= tactic uses a
given equality (which can be a hypothesis, a theorem name, or a complex
term) to "rewrite" the goal. If doing so reduces the goal to an
identity =t = t=, the tactic applies reflexivity to prove it.

Rewrites can applied sequentially, so that the proof above can be
shortened to this:
#+BEGIN_SRC lean
variables (a b c d e : ℕ)
variable h1 : a = b
variable h2 : b = c + 1
variable h3 : c = d
variable h4 : e = 1 + d

include h1 h2 h3 h4
-- BEGIN
theorem T : a = e :=
calc
  a     = d + 1  : by rw [h1, h2, h3]
    ... = 1 + d  : by rw add_comm
    ... =  e     : by rw h4
-- END
#+END_SRC
Or even this:
#+BEGIN_SRC lean
variables (a b c d e : ℕ)
variable h1 : a = b
variable h2 : b = c + 1
variable h3 : c = d
variable h4 : e = 1 + d

include h1 h2 h3 h4
-- BEGIN
theorem T : a = e :=
by rw [h1, h2, h3, add_comm, h4]
-- END
#+END_SRC
The =simp= tactic, instead, rewrites the goal by applying the given
identities repeatedly, in any order, anywhere they are applicable in a
term. It also uses other rules that have been previously declared to
the system, and applies associativity and commutativity wisely to put
expressions in canonical forms. As a result, we can also prove the
theorem as follows:
#+BEGIN_SRC lean
variables (a b c d e : ℕ)
variable h1 : a = b
variable h2 : b = c + 1
variable h3 : c = d
variable h4 : e = 1 + d

include h1 h2 h3 h4
-- BEGIN
theorem T : a = e :=
by simp [h1, h2, h3, h4]
-- END
#+END_SRC
We will discuss variations of =rw= and =simp= in the next chapter.

The =calc= command can be configured for any relation that supports
some form of transitivity. It can even combine different relations.
#+BEGIN_SRC lean
theorem T2 (a b c d : ℕ)
  (h1 : a = b) (h2 : b ≤ c) (h3 : c + 1 < d) : a < d :=
calc
  a     = b     : h1
    ... < b + 1 : nat.lt_succ_self b
    ... ≤ c + 1 : nat.succ_le_succ h2
    ... < d     : h3
#+END_SRC

With =calc=, we can write the proof in the last section in a more
natural and perspicuous way.
#+BEGIN_SRC lean
example (x y : ℕ) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
calc
  (x + y) * (x + y) = (x + y) * x + (x + y) * y  : by rw mul_add
    ... = x * x + y * x + (x + y) * y            : by rw add_mul
    ... = x * x + y * x + (x * y + y * y)        : by rw add_mul
    ... = x * x + y * x + x * y + y * y          : by rw -add_assoc
#+END_SRC
Here the negation before =add_assoc= tells rewrite to use the identity
in the opposite direction. If brevity is what we are after, both =rw=
and =simp= can do the job on their own:
#+BEGIN_SRC lean
example (x y : ℕ) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
by rw [mul_add, add_mul, add_mul, -add_assoc]

example (x y : ℕ) : (x + y) * (x + y) = x * x + y * x + x * y + y * y :=
by simp [mul_add, add_mul]
#+END_SRC

# TODO: when the arithmetic normalizer is working better, give
# examples like (x + y)^2 = x^2 + 2 * x * y + y^2, or (x + y) * (x -
# y) = x^2 - y^2.

** The Existential Quantifier
:PROPERTIES:
  :CUSTOM_ID: The_Existential_Quantifier
:END:

Finally, consider the existential quantifier, which can be written as
either =exists x : α, p x= or =∃ x : α, p x=. Both versions are
actually notationally convenient abbreviations for a more long-winded
expression, =Exists (λ x : α, p x)=, defined in Lean's library.

As you should by now expect, the library includes both an introduction
rule and an elimination rule. The introduction rule is
straightforward: to prove =∃ x : α, p x=, it suffices to provide a
suitable term =t= and a proof of =p t=. here are some examples:
#+BEGIN_SRC lean
open nat

example : ∃ x : ℕ, x > 0 :=
have h : 1 > 0, from zero_lt_succ 0,
exists.intro 1 h

example (x : ℕ) (h : x > 0) : ∃ y, y < x :=
exists.intro 0 h

example (x y z : ℕ) (hxy : x < y) (hyz : y < z) : ∃ w, x < w ∧ w < z :=
exists.intro y (and.intro hxy hyz)

#check @exists.intro
#+END_SRC
We can use the anonymous constructor notation =⟨t, h⟩= for
=exists.intro t h=, when the type is clear from the context.
#+BEGIN_SRC lean
open nat

-- BEGIN
example : ∃ x : ℕ, x > 0 :=
⟨1, zero_lt_succ 0⟩

example (x : ℕ) (h : x > 0) : ∃ y, y < x :=
⟨0, h⟩

example (x y z : ℕ) (hxy : x < y) (hyz : y < z) : ∃ w, x < w ∧ w < z :=
⟨y, hxy, hyz⟩
-- END
#+END_SRC
Note that =exists.intro= has implicit arguments: Lean has to infer the
predicate =p : α → Prop= in the conclusion =∃ x, p x=. This is not a
trivial affair. For example, if we have have =hg : g 0 0 = 0= and
write =exists.intro 0 hg=, there are many possible values for the
predicate =p=, corresponding to the theorems =∃ x, g x x = x=, =∃ x, g
x x = 0=, =∃ x, g x 0 = x=, etc. Lean uses the context to infer which
one is appropriate. This is illustrated in the following example, in
which we set the option =pp.implicit= to true to ask Lean's
pretty-printer to show the implicit arguments.
#+BEGIN_SRC lean
variable g : ℕ → ℕ → ℕ
variable hg : g 0 0 = 0

theorem gex1 : ∃ x, g x x = x := ⟨0, hg⟩
theorem gex2 : ∃ x, g x 0 = x := ⟨0, hg⟩
theorem gex3 : ∃ x, g 0 0 = x := ⟨0, hg⟩
theorem gex4 : ∃ x, g x x = 0 := ⟨0, hg⟩

set_option pp.implicit true  -- display implicit arguments
#check gex1
#check gex2
#check gex3
#check gex4
#+END_SRC

We can view =exists.intro= as an information-hiding operation, since
it hides the witness to the body of the assertion. The existential
elimination rule, =exists.elim=, performs the opposite operation. It
allows us to prove a proposition =q= from =∃ x : α, p x=, by showing
that =q= follows from =p w= for an arbitrary value =w=. Roughly
speaking, since we know there is an =x= satisfying =p x=, we can give
it a name, say, =w=. If =q= does not mention =w=, then showing that
=q= follows from =p w= is tantamount to showing the =q= follows from
the existence of any such =x=. Here is an example:
#+BEGIN_SRC lean
variables (α : Type) (p q : α → Prop)

example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
exists.elim h
  (take w,
    assume hw : p w ∧ q w,
    show ∃ x, q x ∧ p x, from ⟨w, hw.right, hw.left⟩)
#+END_SRC
It may be helpful to compare the exists-elimination rule to the
or-elimination rule: the assertion =∃ x : α, p x= can be thought of as
a big disjunction of the propositions =p a=, as =a= ranges over all
the elements of =α=. Note that the anonymous constructor notation =⟨w,
hw.right, hw.left⟩= abbreviates a nested constructor application; we
could equally well have written =⟨w, ⟨hw.right, hw.left⟩⟩=.

Notice that an existential proposition is very similar to a sigma
type, as described in [[file:02_Dependent_Type_Theory.org::#Dependent_Types][Section 2.8]]. The difference is that given =a :
α= and =h : p a=, the term =exists.intro a h= has type =(∃ x : α, p
x) : Prop= and =sigma.mk a h= has type =(Σ x : α, p x) : Type=. The
similarity between =∃= and =Σ= is another instance of the Curry-Howard
isomorphism.

Lean provides a more convenient way to eliminate from an existential
quantifier with the =match= statement:
#+BEGIN_SRC lean
variables (α : Type) (p q : α → Prop)

example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
match h with ⟨w, hw⟩ :=
  ⟨w, hw.right, hw.left⟩
end
#+END_SRC
The =match= statement is part of Lean's function definition system,
which provides a convenient and expressive ways of defining complex
functions. Once again, it is the Curry-Howard isomorphism that allows
us to co-opt this mechanism for writing proofs as well. The =match=
statement "destructs" the existential assertion into the components
=w= and =hw=, which can then be used in the body of the statement to
prove the proposition. We can annotate the types used in the match for
greater clarity:
#+BEGIN_SRC lean
variables (α : Type) (p q : α → Prop)

-- BEGIN
example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
match h with ⟨(w : α), (hw : p w ∧ q w)⟩ :=
  ⟨w, hw.right, hw.left⟩
end
-- END
#+END_SRC
We can even use the match statement to decompose the conjunction at
the same time:
#+BEGIN_SRC lean
variables (α : Type) (p q : α → Prop)

-- BEGIN
example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
match h with ⟨w, hpw, hqw⟩ :=
  ⟨w, hqw, hpw⟩
end
-- END
#+END_SRC
Lean also provides a pattern-matching =let= expression:
#+BEGIN_SRC lean
variables (α : Type) (p q : α → Prop)

-- BEGIN
example (h : ∃ x, p x ∧ q x) : ∃ x, q x ∧ p x :=
let ⟨w, hpw, hqw⟩ := h in ⟨w, hqw, hpw⟩
-- END
#+END_SRC
This is essentially just alternative notation for the =match=
construct above. Lean will even allow us to use an implicit =match= in
the =assume= statement:
#+BEGIN_SRC lean
variables (α : Type) (p q : α → Prop)

-- BEGIN
example : (∃ x, p x ∧ q x) → ∃ x, q x ∧ p x :=
assume ⟨w, hpw, hqw⟩, ⟨w, hqw, hpw⟩
-- END
#+END_SRC
We will see in [[file:08_Induction_and_Recursion.org::#Induction_and_Recursion][Chapter 8]] that all these variations are instances of a
more general pattern-matching construct.

In the following example, we define =even a= as =∃ b, a = 2*b=, and
then we show that the sum of two even numbers is an even number.
#+BEGIN_SRC lean
def is_even (a : nat) := ∃ b, a = 2 * b

theorem even_plus_even {a b : nat} (h1 : is_even a) (h2 : is_even b) : is_even (a + b) :=
exists.elim h1 (take w1, assume hw1 : a = 2 * w1,
exists.elim h2 (take w2, assume hw2 : b = 2 * w2,
  exists.intro (w1 + w2)
    (calc
      a + b = 2 * w1 + 2 * w2  : by rw [hw1, hw2]
        ... = 2*(w1 + w2)      : by rw mul_add)))
#+END_SRC
Using the various gadgets described in this chapter --- the match
statement, anonymous constructors, and the =rewrite= tactic, we can
write this proof concisely as follows:
#+BEGIN_SRC lean
def is_even (a : nat) := ∃ b, a = 2 * b

-- BEGIN
theorem even_plus_even {a b : nat} (h1 : is_even a) (h2 : is_even b) : is_even (a + b) :=
match h1, h2 with
  ⟨w1, hw1⟩, ⟨w2, hw2⟩ := ⟨w1 + w2, by rw [hw1, hw2, mul_add]⟩
end
-- END
#+END_SRC

Just as the constructive "or" is stronger than the classical "or," so,
too, is the constructive "exists" stronger than the classical
"exists". For example, the following implication requires classical
reasoning because, from a constructive standpoint, knowing that it is
not the case that every =x= satisfies =¬ p= is not the same as having
a particular =x= that satisfies =p=.
#+BEGIN_SRC lean
open classical

variables (α : Type) (p : α → Prop)

example (h : ¬ ∀ x, ¬ p x) : ∃ x, p x :=
by_contradiction
  (assume h1 : ¬ ∃ x, p x,
    have h2 : ∀ x, ¬ p x, from
      take x,
      assume h3 : p x,
      have h4 : ∃ x, p x, from  ⟨x, h3⟩,
      show false, from h1 h4,
    show false, from h h2)
#+END_SRC

What follows are some common identities involving the existential
quantifier. In the exercises below, we encourage you to prove as many
as you can. We also leave it to you to determine which are
nonconstructive, and hence require some form of classical reasoning.
#+BEGIN_SRC lean
open classical

variables (α : Type) (p q : α → Prop)
variable a : α
variable r : Prop

example : (∃ x : α, r) → r := sorry
example : r → (∃ x : α, r) := sorry
example : (∃ x, p x ∧ r) ↔ (∃ x, p x) ∧ r := sorry
example : (∃ x, p x ∨ q x) ↔ (∃ x, p x) ∨ (∃ x, q x) := sorry

example : (∀ x, p x) ↔ ¬ (∃ x, ¬ p x) := sorry
example : (∃ x, p x) ↔ ¬ (∀ x, ¬ p x) := sorry
example : (¬ ∃ x, p x) ↔ (∀ x, ¬ p x) := sorry
example : (¬ ∀ x, p x) ↔ (∃ x, ¬ p x) := sorry

example : (∀ x, p x → r) ↔ (∃ x, p x) → r := sorry
example : (∃ x, p x → r) ↔ (∀ x, p x) → r := sorry
example : (∃ x, r → p x) ↔ (r → ∃ x, p x) := sorry
#+END_SRC
Notice that the declaration =variable a : α= amounts to the assumption
that there is at least one element of type =α=. This assumption is
needed in the second example, as well as in the last two.

Here are solutions to two of the more difficult ones:
#+BEGIN_SRC lean
open classical

variables (α : Type) (p q : α → Prop)
variable a : α
variable r : Prop

-- BEGIN
example : (∃ x, p x ∨ q x) ↔ (∃ x, p x) ∨ (∃ x, q x) :=
iff.intro
  (assume ⟨a, (h1 : p a ∨ q a)⟩,
    or.elim h1
      (assume hpa : p a, or.inl ⟨a, hpa⟩)
      (assume hqa : q a, or.inr ⟨a, hqa⟩))
  (assume h : (∃ x, p x) ∨ (∃ x, q x),
    or.elim h
      (assume ⟨a, hpa⟩, ⟨a, (or.inl hpa)⟩)
      (assume ⟨a, hqa⟩, ⟨a, (or.inr hqa)⟩))

example : (∃ x, p x → r) ↔ (∀ x, p x) → r :=
iff.intro
  (assume ⟨b, (hb : p b → r)⟩,
    assume h2 : ∀ x, p x,
    show r, from  hb (h2 b))
  (assume h1 : (∀ x, p x) → r,
    show ∃ x, p x → r, from
      by_cases
        (assume hap : ∀ x, p x, ⟨a, λ h', h1 hap⟩)
        (assume hnap : ¬ ∀ x, p x,
          by_contradiction
            (assume hnex : ¬ ∃ x, p x → r,
              have hap : ∀ x, p x, from
                take x,
                by_contradiction
                  (assume hnp : ¬ p x,
                    have hex : ∃ x, p x → r,
                      from ⟨x, (assume hp, absurd hp hnp)⟩,
                    show false, from hnex hex),
              show false, from hnap hap)))
-- END
#+END_SRC

** More on the Proof Language

We have seen that keywords like =assume=, =take=, =have=, and =show=
make it possible to write formal proof terms that mirror the
structure of informal mathematical proofs. In this section, we discuss
some additional features of the proof language that are often
convenient.

To start with, we can use anonymous "have" expressions to introduce an
auxiliary goal without having to label it. We can refer to the last
expression introduced in this way using the keyword =this=:
#+BEGIN_SRC lean
variable f : ℕ → ℕ
variable h : ∀ x : ℕ, f x ≤ f (x + 1)

example : f 0 ≤ f 3 :=
have f 0 ≤ f 1, from h 0,
have f 0 ≤ f 2, from le_trans this (h 1),
show f 0 ≤ f 3, from le_trans this (h 2)
#+END_SRC
Often proofs move from one fact to the next, so this can be effective
in eliminating the clutter of lots of labels.

When the goal can be inferred, we can also ask Lean instead to fill in
the proof by writing =by assumption=:
#+BEGIN_SRC lean
variable f : ℕ → ℕ
variable h : ∀ x : ℕ, f x ≤ f (x + 1)

example : f 0 ≤ f 3 :=
have f 0 ≤ f 1, from h 0,
have f 0 ≤ f 2, from le_trans (by assumption) (h 1),
show f 0 ≤ f 3, from le_trans (by assumption) (h 2)
#+END_SRC
This tells Lean to use the =assumption= tactic, which, in turn, proves
the goal by finding a suitable hypothesis in the local context. We
will learn more about the =assumption= tactic in the next chapter.

We can also ask Lean to fill in the proof by writing =‹p›=, where =p=
is the proposition whose proof we want Lean to find in the context.
#+BEGIN_SRC lean
variable f : ℕ → ℕ
variable h : ∀ x : ℕ, f x ≤ f (x + 1)

-- BEGIN
example : f 0 ≥ f 1 → f 1 ≥ f 2 → f 0 = f 2 :=
suppose f 0 ≥ f 1,
suppose f 1 ≥ f 2,
have f 0 ≥ f 2, from le_trans this ‹f 0 ≥ f 1›,
have f 0 ≤ f 2, from le_trans (h 0) (h 1),
show f 0 = f 2, from le_antisymm this ‹f 0 ≥ f 2›
-- END
#+END_SRC
You can type these corner quotes using =\f<= and =\f>=,
respectively. The letter "f" is for "French," since the unicode
symbols can also be used as French quotation marks. In fact, the
notation is defined in Lean as follows:
#+BEGIN_SRC lean
notation `‹` p `›` := show p, by assumption
#+END_SRC
This approach is more robust than using =by assumption=, because the
type of the assumption that needs to be inferred is given
explicitly. It also makes proofs more readable. Here is a more
elaborate example:
#+BEGIN_SRC lean
variable f : ℕ → ℕ
variable h : ∀ x : ℕ, f x ≤ f (x + 1)

-- BEGIN
example : f 0 ≤ f 3 :=
have f 0 ≤ f 1, from h 0,
have f 1 ≤ f 2, from h 1,
have f 2 ≤ f 3, from h 2,
show f 0 ≤ f 3, from le_trans ‹f 0 ≤ f 1›
                       (le_trans ‹f 1 ≤ f 2› ‹f 2 ≤ f 3›)
-- END
#+END_SRC
Keep in mind that use can use the French quotation marks in this way
to refer to /anything/ in the context, not just things that were
introduced anonymously. Its use is also not limited to propositions,
though using it for data is somewhat odd:
#+BEGIN_SRC lean
example (n : ℕ) : ℕ := ‹ℕ›
#+END_SRC

The =suppose= keyword acts as an anonymous assume:
#+BEGIN_SRC lean
variable f : ℕ → ℕ
variable h : ∀ x : ℕ, f x ≤ f (x + 1)

-- BEGIN
example : f 0 ≥ f 1 → f 0 = f 1 :=
suppose f 0 ≥ f 1,
show f 0 = f 1, from le_antisymm (h 0) this
-- END
#+END_SRC
Notice that there is an asymmetry: you can use =have= with or without
a label, but if you do not wish to name the assumption, you must use
=suppose= rather than =assume=. The reason is that Lean allows us to
write =assume h= to introduce a hypothesis without specifying it,
leaving it to the system to infer to relevant assumption. An anonymous
=assume= would thus lead to ambiguities when parsing expressions.

As with the anonymous =have=, when you use =suppose= to introduce an
assumption, that assumption can also be invoked later in the proof by
enclosing it in French quotes.
#+BEGIN_SRC lean
variable f : ℕ → ℕ
variable h : ∀ x : ℕ, f x ≤ f (x + 1)

-- BEGIN
example : f 0 ≥ f 1 → f 1 ≥ f 2 → f 0 = f 2 :=
suppose f 0 ≥ f 1,
suppose f 1 ≥ f 2,
have f 0 ≥ f 2, from le_trans ‹f 2 ≤ f 1› ‹f 1 ≤ f 0›,
have f 0 ≤ f 2, from le_trans (h 0) (h 1),
show f 0 = f 2, from le_antisymm this ‹f 0 ≥ f 2›
-- END
#+END_SRC
Notice that =le_antisymm= is the assertion that if =a ≤ b= and =b ≤ a=
then =a = b=, and =a ≥ b= is definitionally equal to =b ≤ a=.

# TODO: include a nice example like this when possible.

# The following proof that the square root of two is irrational can be
# found in the standard library. It provides a nice example of the way that
# proof terms can be structured and made readable using the devices we have
# discussed here.
# #+BEGIN_SRC lean
# import data.nat
# open nat

# theorem sqrt_two_irrational {a b : ℕ} (co : coprime a b) : a^2 ≠ 2 * b^2 :=
# assume h : a^2 = 2 * b^2,
# have even (a^2),
#   from even_of_exists (exists.intro _ h),
# have even a,
#   from even_of_even_pow this,
# obtain (c : ℕ) (aeq : a = 2 * c),
#   from exists_of_even this,
# have 2 * (2 * c^2) = 2 * b^2,
#   by rewrite [-h, aeq, *pow_two, mul.assoc, mul.left_comm c],
# have 2 * c^2 = b^2,
#   from eq_of_mul_eq_mul_left dec_trivial this,
# have even (b^2),
#   from even_of_exists (exists.intro _ (eq.symm this)),
# have even b,
#   from even_of_even_pow this,
# have 2 ∣ gcd a b,
#   from dvd_gcd (dvd_of_even `even a`) (dvd_of_even `even b`),
# have 2 ∣ (1 : ℕ),
#   by rewrite [gcd_eq_one_of_coprime co at this]; exact this,
# show false, from absurd `2 ∣ 1` dec_trivial
# #+END_SRC

# A previously used example is found below. We had to reject it because
# we cannot currently include primes.lean in the web version of Lean.
# See the discussion at https://github.com/leanprover/tutorial/issues/138.
#
# The following proof that there are infinitely many primes is a slight
# variant of the proof in the standard library. It provides a nice
# example of the way that proof terms can be structured and made
# readable using the various devices we have discussed here.
# #+BEGIN_SRC lean
# import theories.number_theory.primes
# open nat decidable eq.ops

# theorem primes_infinite (n : nat) : ∃ p, p ≥ n ∧ prime p :=
# let m := fact (n + 1) in
# have m ≥ 1,     from le_of_lt_succ (succ_lt_succ (fact_pos _)),
# have m + 1 ≥ 2, from succ_le_succ this,
# obtain p `prime p` `p ∣ m + 1`, from sub_prime_and_dvd this,
# have p ≥ 2, from ge_two_of_prime `prime p`,
# have p > 0, from pos_of_prime `prime p`,
# have p ≥ n, from by_contradiction
#   (suppose ¬ p ≥ n,
#     have p < n,     from lt_of_not_ge this,
#     have p ≤ n + 1, from le_of_lt (lt.step this),
#     have p ∣ m,      from dvd_fact `p > 0` this,
#     have p ∣ 1,      from dvd_of_dvd_add_right (!add.comm ▸ `p ∣ m + 1`) this,
#     have p ≤ 1,     from le_of_dvd zero_lt_one this,
#     absurd (le.trans `2 ≤ p` `p ≤ 1`) dec_trivial),
# exists.intro p (and.intro this `prime p`)
# #+END_SRC

** Exercises

1. Prove these equivalences:
   #+BEGIN_SRC lean
   variables (α : Type) (p q : α → Prop)

   example : (∀ x, p x ∧ q x) ↔ (∀ x, p x) ∧ (∀ x, q x) := sorry
   example : (∀ x, p x → q x) → (∀ x, p x) → (∀ x, q x) := sorry
   example : (∀ x, p x) ∨ (∀ x, q x) → ∀ x, p x ∨ q x := sorry
   #+END_SRC
   You should also try to understand why the reverse implication is not
   derivable in the last example.

2. It is often possible to bring a component of a formula outside a universal
   quantifier, when it does not depend on the quantified variable. Try
   proving these (one direction of the second of these requires classical
   logic):
   #+BEGIN_SRC lean
   variables (α : Type) (p q : α → Prop)
   variable r : Prop

   example : α → ((∀ x : α, r) ↔ r) := sorry
   example : (∀ x, p x ∨ r) ↔ (∀ x, p x) ∨ r := sorry
   example : (∀ x, r → p x) ↔ (r → ∀ x, p x) := sorry
   #+END_SRC

3. As a final example, consider the "barber paradox," that is, the claim
   that in a certain town there is a (male) barber that shaves all and only the
   men who do not shave themselves. Prove that this implies a
   contradiction:
   #+BEGIN_SRC lean
   variables (men : Type) (barber : men) (shaves : men → men → Prop)

   example (h : ∀ x : men, shaves barber x ↔ ¬ shaves x x) : false := sorry
   #+END_SRC

4. Below, we have put definitions of =divides= and =even= in a special
   namespace to avoid conflicts with definitions in the library. The
   =instance= declaration make it possible to use the notation =m | n=
   for =divides m n=. Don't worry about how it works; you will learn
   about that later.
   #+BEGIN_SRC lean
   namespace hide

   def divides (m n : ℕ) : Prop := ∃ k, m * k = n

   instance : has_dvd nat := ⟨divides⟩

   def even (n : ℕ) : Prop := 2 ∣ n

   section
     variables m n : ℕ

     #check m ∣ n
     #check m^n
     #check even (m^n +3)
   end

   end hide
   #+END_SRC
   Remember that, without any parameters, an expression of type =Prop=
   is just an assertion. Fill in the definitions of =prime= and
   =Fermat_prime= below, and construct the given assertion. For
   example, you can say that there are infinitely many primes by
   asserting that for every natural number =n=, there is a prime
   number greater than =n=. Goldbach's weak conjecture states that
   every odd number greater than 5 is the sum of three primes. Look
   up the definition of a Fermat prime or any of the other statements,
   if necessary.
   #+BEGIN_SRC lean
   namespace hide

   def divides (m n : ℕ) : Prop := ∃ k, m * k = n

   instance : has_dvd nat := ⟨divides⟩

   def even (n : ℕ) : Prop := 2 ∣ n

   -- BEGIN
   def prime (n : ℕ) : Prop := sorry

   def infinitely_many_primes : Prop := sorry

   def Fermat_prime (n : ℕ) : Prop := sorry

   def infinitely_many_Fermat_primes : Prop := sorry

   def goldbach_conjecture : Prop := sorry

   def Goldbach's_weak_conjecture : Prop := sorry

   def Fermat's_last_theorem : Prop := sorry
   -- END

   end hide
   #+END_SRC

5. Prove as many of the identities listed in [[#The_Existential_Quantifier][Section 4.4]] as you can.

6. Give a calculational proof of the theorem =log_mul= below.
   #+BEGIN_SRC lean
   variables (real : Type) [ordered_ring real]
   variables (log exp : real → real)
   variable  log_exp_eq : ∀ x, log (exp x) = x
   variable  exp_log_eq : ∀ {x}, x > 0 → exp (log x) = x
   variable  exp_pos    : ∀ x, exp x > 0
   variable  exp_add    : ∀ x y, exp (x + y) = exp x * exp y

   -- this ensures the assumptions are available in tactic proofs
   include log_exp_eq exp_log_eq exp_pos exp_add

   example (x y z : real) : exp (x + y + z) = exp x * exp y * exp z :=
   by rw [exp_add, exp_add]

   example (y : real) (h : y > 0)  : exp (log y) = y := exp_log_eq h

   theorem log_mul {x y : real} (hx : x > 0) (hy : y > 0) : log (x * y) = log x + log y :=
   sorry
   #+END_SRC

7. Prove the theorem below, using only the ring properties of =ℤ=
   enumerated in [[#Equality][Section 4.2]] and the theorem =sub_self=.
   #+BEGIN_SRC lean
   #check sub_self

   example (x : ℤ) : x * 0 = 0 :=
   sorry
   #+END_SRC

# TODO: in exercise 5, replace the next example with actual theorems when they exist.


